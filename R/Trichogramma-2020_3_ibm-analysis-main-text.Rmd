---
title: 'Trichogramma range expansions (genetics and dynamics): 3- individual-based model, analysis (main text)'
author: "Maxime Dahirel"
date:
output: 
  html_document:
    theme: yeti
    toc: TRUE
    toc_float: TRUE
    code_download: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, eval = FALSE)
```

```{r load-packages}
library(arm) # for invlogit

library(tidyverse)

library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = 2)
library(brms)
library(bayesplot)
library(tidybayes)

library(cowplot)
library(patchwork)

library(here)
```

# Introduction

(see manuscript/preprint text, as well as scripts 1 and 2 in this folder, for more context)

Pushed expansion waves are generated when spread is positively density-dependent, i.e. when dispersal and/or growth is positively density-dependent. Pushed waves have mostly been studied using the latter (which corresponds to Allee effects), but recent works have started to focus on density-dependent dispersal, as it is common in nature. We propose to expand on the effects of variable dispersal by looking at the links between connectivity and stochasticity in dispersal. We predict that (at low population sizes) reduced connectivity should lead to increased stochasticity in dispersal success and establishment difficulties, creating positive density-dependent spread through another way. 

# Analysis

## Loading dataset

We simulated a total of 1000 range expansions using Netlogo (2 equilibrium population sizes x 5 scenarios), let's import them:

```{r importing-data}
data <- read_csv(file = here("NetLogo_output/model-output.csv"))
```

This dataset contains the following variables (see script 2 for how they were obtained):

- `ticks` : number of generations since release

- `start_allee_thres`, `slope_disp_mean`, `disp0_mean`: Allee threshold and density-dispersal reaction norm parameters underlying the different scenarios used (described in `treatment`)

- `fecundity` : (theoretical) average fecundity at lowest density in the absence of Allee effect; should be fixed to 5 in present analysis

- `K` : equilibrium population density in core patches; either 225 or 450

- `seedID`: random seed ID. Together with `start_allee_thres`, `slope_disp_mean`, `disp0_mean`, uniquely identify a replicate (see `replicateID`)

- `pxcor`: coordinate of the focal patch, in number of patches from origin

- `N_predispersal` and `Npostdispersal`: number of individuals in the focal patch before or after the dispersal phase of the life cycle

- `N_allele0` and `N_allele1`: number of `Ndispersal` individuals bearing the neutral allele 0 or 1 (`P0` and `P1` are the corresponding allelic proportions)

- `is.edge`: whether the focal patch is the most advanced one at the time ("edge", TRUE), or the initial, release patch ("core", FALSE). Combine with `ReplicateID`, define a time series (see `Location_full`)

-  `Hexp`: patch genetic diversity (expected heterozygosity)

- `v_f`: expected velocity of a pulled wave with identical fecundity and dispersal at low density as the focal one (estimated here for population density = 1)

## Data processing

### Dataset front: speed of the advancing range expansions

We here simply (1) remove the core patches, (2) do some renaming:

```{r data-front}
data_front <- data %>%
  filter(is.edge == TRUE) %>%
  mutate(front = pxcor) %>%
  mutate(K = factor(K)) %>% 
  mutate(treatment = fct_recode(treatment,`weak Allee effect` = "weak Allee effect (a = 0.95)"))

```


### Dataset genetics: dynamics of neutral genetic diversity during expansion

The situation here is bit more complex. We can't simply use the heterozygosity per patch as in the experimental dataset, because there are many zeroes:
```{r data-genetics1}
 ggplot(data) +geom_line(aes(x=ticks,y=Hexp,group=replicateID,col=treatment))+facet_wrap(~treatment+is.edge+K)
### lots of zero, can't use
```
and zeroes don't work well with the stat methods we want to use (which involve logits to accurately deal with % data, which heterozygosity data are _ and using zero-inflated methods while using a non-linear theoretical formula is ... non-intuitive).

Fortunately, there is a way around this that doesn't involve giving up and modelling as a gaussian (which would lead to wrongly distributed errors): by using the among replicates variance in allelic frequencies rather than using heterozygosity directly (see Gandhi et al. 2019 PNAS)
```{r data-genetics2}
#### we'll use method variance in fraction (gandhi 2019 pnas).
data_genetics <- data %>%
  group_by(ticks, treatment, is.edge, K) %>% # we group by "type" of patch and generation
  summarise(varP1 = var(P1), varP0 = var(P0)) %>% # we can use either. They are going to be the same for a biallelic locus: var = p(1-p)
  mutate(K = factor(K)) %>% 
  ungroup() %>% 
  mutate(treatment = fct_recode(treatment,`weak Allee effect` = "weak Allee effect (a = 0.95)"))

```

Now we have everything we need, we can fit some models! (see manuscript for details)

## Data analysis

### Dataset front: speed of the advancing range expansions

Like in script 1, we here assume each front initially advances at a speed that may not be the asymptotic/equilibrium speed (which is what interests us), due to various stochastic reasons, but end up converging to it more or less exponentially. Again, we here model front *location* as a function of time, rather than speed directly:

```{r model-front-setup}
prior_front_IBM <- c(
  # set_prior("normal(0,1)",nlpar="logitspeedstart",class="b"), #prior not needed because we fix start speed to 1
  set_prior("normal(0,1.5)", nlpar = "logitspeedasym", class = "b"),
  set_prior("normal(0,1)", nlpar = "lograte", class = "b"),
  set_prior("normal(0,1)", nlpar = "logitspeedasym", class = "sd"),
  set_prior("normal(0,1)", nlpar = "lograte", class = "sd"),
  set_prior("normal(0,1)", class = "sigma"),
  set_prior("lkj(2)", class = "cor")
)

bf_front_IBM <- bf(front ~ log(speed * ticks),
  nlf(speed ~ speedasym + (1 - speedasym) * exp(-((ticks - 1) / 10) * rate)), ## speedstart forced to 1
  nlf(rate ~ 10^(lograte)),
  nlf(speedasym ~ inv_logit(logitspeedasym)),
  logitspeedasym ~ 0 + treatment:K + (1 | 1 | replicateID),
  lograte ~ 0 + treatment:K + (1 | 1 | replicateID),
  family = lognormal, nl = TRUE
)
```
(the division of `ticks` by 10 is just a rescaling trick to make convergence of the `rate` parameter faster)

To save computing time (given the very large dataset), we are also only using every 5th `tick` to fit the model (it's trivial to see this has little to no effect on predictive success, using e.g. `bayes_R2` to compare success on in-model and out-of-model data):

```{r model-front}

data_front_part <- data_front %>% filter((ticks %% 5) == 0)

if (file.exists(here("R_output", "model5_front_IBM.Rdata"))) {
  load(here("R_output", "model5_front_IBM.Rdata"))
} else {
  mod_front_IBM <- brm(bf_front_IBM,
    data = data_front_part,
    chains = 4, iter = 6000, warmup = 2000,
    prior = prior_front_IBM,
    seed = 42
  )

  
### and now the LOO-PSIS to compare this model with the "power decay" model later
# loo_mod_front_IBM <- loo(mod_front_IBM)
### this model is BIG, if you run into memory problem, a solution is to compute the log-likelihood for only part of the samples
  loo_mod_front_IBM<-loo(mod_front_IBM,
                       nsamples=4000,subset=1:4000 #,## 1/4 of posterior samples; 
                       #comparison with power model gives same conclusions when using any of the other 1/4
                       )


  save(list = c("mod_front_IBM","loo_mod_front_IBM"), file = here("R_output", "model5_front_IBM.Rdata"))
}
```


But we need to test the possibility that a power decay is more appropriate

```{r model-front-power}


bf_front_IBM2 <- bf(front ~ log(speed * ticks),
               nlf(speed ~ speedasym + (1 - speedasym) * (ticks) ^(-rate)), ## speedstart forced to 1
               nlf(rate ~ 10^(lograte)),
               nlf(speedasym ~ inv_logit(logitspeedasym)),
               logitspeedasym ~ 0 + treatment:K + (1 | 1 | replicateID),
               lograte ~ 0 + treatment:K + (1 | 1 | replicateID),
               family = lognormal, nl = TRUE
)

if (file.exists(here("R_output", "model5bis_front_power_IBM.Rdata"))) {
  load(here("R_output", "model5bis_front_power_IBM.Rdata"))
} else {

mod_front_IBM2 <- brm(bf_front_IBM2,
                 data = data_front_part,
                 chains = 4, iter = 6000, warmup = 2000,
                 prior = prior_front_IBM,
                 seed = 42
)

  loo_mod_front_IBM2<-loo(mod_front_IBM2,
                       nsamples=4000,subset=1:4000 #,## 1/4 of posterior samples; 
                       #comparison with power model gives same conclusions when using any of the other quarters
                       )

  save(list = c("mod_front_IBM2","loo_mod_front_IBM2"), file = here("R_output", "model5bis_front_power_IBM.Rdata"))
}

```


```{r model-front-comparison}
loo_compare(loo_mod_front_IBM,loo_mod_front_IBM2)
  # there's some "problematic" observations in both, but they are rare, and using moment_match to account for them leads to very high
  # computation time and memory requirements
  # and there are too many problematic obs to use reloo
  # so we'll have to roll with them as is
  # the difference between the two models is so big, and the "problematic" obs so rare it shouldn't be a problem

#but let's still split the elpd in problematic and non-problematic obs
#to see if the difference is caused by the problematic obs
tibble(is.problematic= loo_mod_front_IBM$pointwise[,"influence_pareto_k"]>0.7 | loo_mod_front_IBM2$pointwise[,"influence_pareto_k"]>0.7,
       pointwise_exp =loo_mod_front_IBM$pointwise[,1],
       pointwise_power = loo_mod_front_IBM2$pointwise[,1]) %>% 
       group_by(is.problematic) %>% 
       summarise(elpd_exp=sum(pointwise_exp),
                 elpd_power=sum(pointwise_power),
                 elpd_diff=sum(pointwise_exp-pointwise_power),
                 se_diff=sqrt(length(pointwise_exp))*sd(pointwise_exp-pointwise_power))
                 ## see Vehtari et al 2017 doi:10.1007/s11222-016-9696-4 for the SE formula
#nope, the problematic obs don't cause the differences in elpd; the exponential model still has the clear advantage


##if we do this:
tibble(is.problematic_exp= loo_mod_front_IBM$pointwise[,"influence_pareto_k"]>0.7,
       is.problematic_power=loo_mod_front_IBM2$pointwise[,"influence_pareto_k"]>0.7) %>%
  mutate(ticks=data_front_part$ticks,
         treatment=data_front_part$treatment,
         K=data_front_part$K) %>% 
  ggplot()+
  geom_boxplot(aes(is.problematic_exp,ticks,col=K))+facet_wrap(~treatment)

#we see the problematic obs are limited to the very beginning of the time series, so our model may not be appropriate for these, but should be OK for the asymptotic speed?
#maybe because we model a decay of speed starting immediately while there may be some delay at the start?



```

The exponential model is better supported by far, so we go forward with it rather than the power one, for the rest of the analysis.

```{r summary-model-front}

summary(mod_front_IBM)

### The default summary prints posterior means and quantile intervals. What if we want HD Intervals instead?
summary_front_IBM <- mod_front_IBM %>%
  posterior_samples() %>%
  select(starts_with(c("Intercept", "b_", "sd_", "cor_", "sigma"))) %>%
  pivot_longer(everything()) %>%
  group_by(name) %>%
  mean_hdi() %>%
  print(n = Inf)

mcmc_rank_overlay(mod_front_IBM, pars = summary_front_IBM$name)
```

### Dataset genetics: dynamics of neutral genetic diversity during expansion

Our model here is based on Gandhi et al 2019 PNAS and assumes genetic among replicate variance increases through time until reaching a plateau only determined by the initial allelic frequencies. The "decay" rate parameter is our parameter of interest here, and because we *know* the (average) initial allelic frequencies, we can put a more informative prior on the plateau parameter:

```{r}
if (file.exists(here("R_output", "model6_genetics_IBM.Rdata"))) {
  load(here("R_output", "model6_genetics_IBM.Rdata"))
} else {
  prior_genetics_IBM <- c(
    set_prior("normal(-1.1,0.5)", nlpar = c("logitVmax")), # informative prior (logit(0.25)=-1.10 (rounded to 2 decimals))
    # we *know* Vmax should on average be 0.25 (see main text), but rather than fix it
    # we want a prior with some wiggle room because of the effects of sampling variation (we don't know how pronounced they may be)
    # this prior is broad, counts values of Vmax between 0.1 and 0.4-0.5 as plausible
    # note that results are insensitive to changing that prior for a less informative one
    set_prior("normal(0,1)", nlpar = c("logdecay"), class = "b"),
    set_prior("normal(0,1)", nlpar = "invphi", lb = 0)
  )

  bf_genetics_IBM <- bf(varP1 ~ logit(Vmax * (1 - exp(-(ticks) * decay))),
    nlf(Vmax ~ inv_logit(logitVmax)),
    nlf(decay ~ 10^(logdecay)),
    logitVmax ~ 1,
    logdecay ~ 0 + treatment:is.edge:K,
    nlf(phi ~ 1 / invphi),
    invphi ~ 1,
    family = Beta(link_phi = "identity"), nl = TRUE
  )

  mod_genetics_IBM <- brm(bf_genetics_IBM,
    data = data_genetics,
    iter = 4000, warmup = 2000, chains = 4,
    prior = prior_genetics_IBM, seed = 42
  )

  save(list = "mod_genetics_IBM", file = here("R_output", "model6_genetics_IBM.Rdata"))
}
```

```{r summary-model-genetics}
summary(mod_genetics_IBM)

summary_genetics_IBM <- mod_genetics_IBM %>%
  posterior_samples() %>%
  select(starts_with(c("b_"))) %>%
  pivot_longer(everything()) %>%
  group_by(name) %>%
  mean_hdi() %>%
  print(n = Inf)

mcmc_rank_overlay(mod_genetics_IBM, pars = summary_genetics_IBM$name)

### prediction intervals around each point
ppc_ribbon(
  yrep = predict(mod_genetics_IBM, summary = FALSE),
  x = rank(predict(mod_genetics_IBM)[, 1]),
  y = data_genetics$varP1,
  prob = 0.5, prob_outer = 0.95
)
```

## Postprocessing : plots and summaries

### Dataset front: speed of the advancing range expansions

First let's make a file that contains the speeds at which we move from one type of expansion to the other, for each treatment

```{r velocity-thresholds}
thresholds <- data_front %>%
  mutate(treatment = relevel(factor(treatment), ref = "reference")) %>%
  mutate(threshold_pushed = (3 / (2 * sqrt(2))) * v_f) %>%
  select(K, treatment, v_f, threshold_pushed) %>%
  distinct() %>%
  pivot_longer(
    cols = c(v_f, threshold_pushed),
    values_to = "threshold_value", names_to = "threshold_type"
  ) %>%
  mutate(threshold_type = fct_recode(threshold_type,
    vF = "v_f",
    `3/(2 x sqrt(2))) x vF` = "threshold_pushed"
  ))
```

then do the figure itself:

```{r figure-velocity}
newdata <- data_front_part %>%
  ungroup() %>%
  mutate(treatment = relevel(factor(treatment), ref = "reference")) %>%
  mutate(treatment = fct_relevel(treatment, "reduced connectivity", after = Inf)) %>%
  mutate(treatment = fct_relevel(treatment, "reduced + DDD", after = Inf)) 

newdata %>%
  mutate(replicateID = replicateID[1], ticks = 1) %>%
  select(treatment, ticks, replicateID, K) %>%
  distinct() %>%
  add_fitted_draws(mod_front_IBM, nlpar = "logitspeedasym", re_formula = NA) %>%
  mutate(.value = invlogit(.value)) %>% 
  ungroup() %>%
  ggplot() +
  geom_jitter(data=subset(newdata,ticks==100),  #let's plot the observed speeds at t=100, for illustration
              aes(factor(K),front/ticks),col="grey",alpha=0.5)+
  geom_hline(data = thresholds, aes(yintercept = threshold_value, lty = threshold_type)) +
  stat_eye(aes(x = factor(K), group = K, y = .value), normalize = "xy", .width=c(0.001,0.95)) +
  scale_y_continuous(expression(paste("Mean asymptotic velocity  ", italic(v)," (posterior)"))) +
  scale_x_discrete(expression(paste("equilibrium population size ", italic(K)))) +
  scale_linetype_discrete(
    name = "",
    labels = c(
      expression(paste(frac(3, 2 * sqrt(2)), "  ", italic(v[F]))),
      expression(italic(v[F]))
    )
  ) +
  facet_grid(cols = vars(treatment)) +
  theme_half_open(11) +
  background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.title = element_blank(), legend.position = c(0.86, 0.3), legend.background = element_rect(fill = "white", colour = "black"), legend.margin = margin(t = 2, r = 2, l = 2, b = 2))
```

Finally, let's look at the average velocity ratios for each scenario:

```{r velocity-ratios}
data_front %>%
  ungroup() %>%
  mutate(replicateID = replicateID[1], ticks = 1) %>%
  select(treatment, ticks, replicateID, K, v_f) %>%
  distinct() %>%
  add_fitted_draws(mod_front_IBM, nlpar = "logitspeedasym", re_formula = NA) %>%
  mutate(.value = invlogit(.value)) %>%
  mutate(.value = .value / v_f) %>%
  mean_hdi()
```

### Dataset genetics: dynamics of neutral genetic diversity during expansion

```{r figure-genetics}
predicted_lambda<- data_genetics %>%
  ungroup() %>%
  mutate(treatment = relevel(factor(treatment), ref = "reference")) %>%
  mutate(treatment = fct_relevel(treatment, "reduced connectivity", after = Inf)) %>%
  mutate(treatment = fct_relevel(treatment, "reduced + DDD", after = Inf)) %>%
  select(treatment, is.edge, K) %>%
  mutate(Location = factor(is.edge)) %>%
  mutate(Location = fct_recode(Location, edge = "TRUE", core = "FALSE")) %>%
  mutate(Location = fct_relevel(Location, "edge", "core")) %>%
  distinct() %>%
  mutate(ticks = 1) %>%
  add_fitted_draws(mod_genetics_IBM, re_formula = NA, nlpar = "logdecay") %>%
  mutate(.value = 10^(.value)) %>%
  ungroup()


p1 <- predicted_lambda %>% 
  filter(Location == "edge") %>% 
  ggplot() +
  stat_eye(aes(x = K, y = .value), position = "dodge", normalize = "xy", fill = "#f1a340", .width=c(0.001,0.95)) +
  facet_grid(cols = vars(treatment), scales = "free_y") +
  scale_y_continuous(name = expression(paste("Mean genetic diversity decay rate  ", lambda)))+
  scale_x_discrete("equilibrium population size K") +
  theme_half_open(11) +
  labs(title="edge patches:") + 
  background_grid(colour.major = "grey95", colour.minor = "grey95")

p2 <- predicted_lambda %>% 
  filter(Location == "core") %>% 
  ggplot() +
  stat_eye(aes(x = K, y = .value), position = "dodge", normalize = "xy", fill= "#998ec3", .width=c(0.001,0.95)) +
  facet_grid(cols = vars(treatment), scales = "free_y") +
  scale_y_continuous(name = expression(paste("Mean genetic diversity decay rate  ", lambda)))+
  scale_x_discrete("equilibrium population size K") +
  theme_half_open(11) +
  labs(title="core patches:") +
  background_grid(colour.major = "grey95", colour.minor = "grey95")

p1/p2 + plot_annotation(tag_levels = 'A')
```
